# Celebrity Facial Recognition

## faceScrub Folder
The faceScrub folder is taken as-is from the FaceScrub dataset I received upon inquiry to the owners of the dataset. It is comprised of two text files (the dataset for actors and actresses with names, urls, and hashes), a LICENSE.txt file, and a README.txt file.

## Data Folder
The data folder is the home of all data generated by the Jupyter notebook as part of this project. It has two folders, actors and actresses, with respective subdirectories for each class identified to be used in classification. Classifiers.clf is a file which contains the classifiers and can be accessed by the "Build and Train Classifier" section of the notebook via setting load to True. test_dataset.npz and train_dataset.npz are each files which contain the HOG features and labels for the test and train subsets of data. They are accessed by the "Build and Train Classifier" section of the notebook when load is set to False, in order to train classifiers based on that data. label_map.json is a dictionary of names to integers for easy labeling. Any folders and files in the data folder, including the data folder itself, are regenerated by running the entire notebook with load = False.

## Jupyter Notebook
The Jupyter Notebook containing the project is structured as follows. Detailed comments are included in every code block to explain exactly what happens at each step.
1. Imports
    - os, 
    - hashlib, 
    - requests, 
    - json, 
    - cv2, 
    - numpy, 
    - pickle, 
    - random, 
    - matplotlib.pyplot,
    - math,
    - from collections, defaultdict, Counter,
    - from tqdm, tqdm,
    - from sklearn, svm, metrics
2. Download Images
    - In my tests for the technical report, up to 100 images per class are used, depending on how many valid images are in the dataset.
    - Edit the number of actors and actresses classified by changing the k argument in lines 84, 85.
    - Edit the number of pictures downloaded per class by changing the pictures_per_name argument in lines 88, 89.
    - The program will download and verify images until there are pictures_per_name images in the folder. If images have previously been downloaded, they are included.
    - vis1() shows a grid with one image per class, labeled.
3. HOG Feature Extraction and Database Preparation
    - Use build_datasets to call each other function in the proper order. This will save the test_ and train_dataset.npz files to the data folder.
    - Adjust the output_size argument on line 212 to change the size of images HOG features are extracted from.
    - Adjust the cell_size, block_size, orientation_count, or epsilon arguments on line 52 to change the respective variable in the HOG extraction.
    - vis2() shows a grid with an example of the adjustments made to each image as it is fed into the HOG feature extractor
4. Build and Train Classifier
    - Adjust load variable on line 61 to determine whether to (FALSE) retrain new classifiers and save to data/classifiers.clf or (TRUE) load the existing classifier from data/classifiers.clf
    - metrics.classification_report and metrics.confusion_matrix display the respective metrics below the cell.
    - vis3() displays the training images for Steve Carell, the true positive detections, false negative detections, and false positive detections for him.

NOTE: For more detailed information about implementation, reference comments in Jupyter noteboook. For a higher-level understanding of the pipeline, reference the Final Report. To retain a reasonable file size and upload time there is no data folder. Image, Dataset and classifier files can be made easily by running all cells in order. This should take very little time with such a small number of images.
